import os
import pandas as pd
import numpy as np
import tensorflow as tf
import random
import warnings
from Neural_Network.Models.ModelFunctions import prepare, load_data
from Neural_Network.Build_Network import multivariate_model as Model
from panelsplit.cross_validation import PanelSplit
from panelsplit.plot import plot_splits

warnings.filterwarnings("ignore")
os.environ['PYTHONHASHSEED'] = str(0)




# panel_split = PanelSplit(periods=growth_global['Year'], n_splits=5, gap=0, test_size=1)

# panel_split.u_periods_cv = panel_split._u_periods_cv
# plot_splits(panel_split)



# # (Optional) display splits for verification:
# for train_idx, test_idx in panel_split.split():
#     print("Train:"); display(growth['global'].loc[train_idx])
#     print("Test:"); display(growth['global'].loc[test_idx])




def main_loop(node_index, Model_selection, nodes_list, no_inits, seed_value, lr, min_delta, patience, verbose, dropout, n_splits, formulation):

    # Determine the model and load data accordingly
    if Model_selection == 'CV':
        growth, precip, temp, panel_split = load_data('CV', n_splits, formulation)
    elif Model_selection == 'IC':
        growth, precip, temp= load_data('IC', n_splits, formulation)
    else:
        raise ValueError("Invalid Model_selection. Use 'CV' or 'IC'.")

    # List to save model instances and performance metrics from each initialization.
    models_tmp =  np.zeros(no_inits, dtype=object)

    # === Cross Validation case ===
    if Model_selection == 'CV':
        cv_errors_inits = np.zeros(no_inits)
        # Loop over initializations
        for j in range(no_inits):
            errors_j = []  # Store error (MSE) for each split in this initialization
            current_seed = seed_value + j  # update seed
            # iterate over each split generated by PanelSplit
            for train_idx, test_idx in panel_split.split():
                # Prepare training and test sets
                growth_train = {'global': growth['global'].loc[train_idx]}
                # Reshape test target as needed; adjust reshape parameters if needed
                growth_test = np.array(growth['global'].loc[test_idx].reset_index(drop=True)).reshape((1, 1, -1, 1))

                # For temperature and precipitation:
                temp_train = {'global': temp['global'].loc[train_idx].reset_index(drop=True)}
                temp_test = np.array(temp['global'].loc[test_idx].reset_index(drop=True)).reshape((1, 1, -1, 1))

                precip_train = {'global': precip['global'].loc[train_idx].reset_index(drop=True)}
                precip_test = np.array(precip['global'].loc[test_idx].reset_index(drop=True)).reshape((1, 1, -1, 1))

                x_train = [temp_train, precip_train]
                x_test = tf.concat([temp_test, precip_test], axis=3)

                # Clear session and set seeds for reproducibility
                tf.keras.backend.clear_session()
                tf.random.set_seed(current_seed)
                np.random.default_rng(current_seed)
                random.seed(current_seed)

                # Initialize and fit the model
                model_instance = Model(nodes=nodes_list[node_index], x_train=x_train, y_train=growth_train, dropout=dropout)
                model_instance.fit(lr=lr, min_delta=min_delta, patience=patience, verbose=verbose)
                # Save this instance so that it can be saved later if it performs the best
                models_tmp[j] = model_instance

                # Predict on the test set and calculate MSE
                preds = np.reshape(model_instance.model_pred.predict(x_test), (-1, 1), order='F')
                mse = np.nanmean((preds - growth_test) ** 2)
                errors_j.append(mse)

            # Average error for this initialization
            cv_errors_inits[j] = np.mean(errors_j)
            print(f"Process {os.getpid()} completed initialization {j+1}/{no_inits} for node {nodes_list[node_index]}", flush=True)

        # Select best initialization (i.e., the one with the lowest average MSE)
        best_init_idx = int(np.argmin(cv_errors_inits))
        best_cv_error = cv_errors_inits[best_init_idx]

        train_on_full_sample(best_init_idx, nodes_list[node_index], growth, precip, temp, lr, min_delta, patience, verbose, dropout)

        return best_cv_error, nodes_list[node_index]

    # === Information Criteria case ===
    elif Model_selection == 'IC':
        #fit a model for each key in the growth dictionary
        x_train = {'temp':temp, 'precip':precip}
        BIC_list = np.zeros(no_inits)
        AIC_list = np.zeros(no_inits)
        # Loop over each initialization
        for j in range(no_inits):
            current_seed = seed_value + j  # update seed
            tf.keras.backend.clear_session()
            tf.random.set_seed(current_seed)
            np.random.default_rng(current_seed)
            random.seed(current_seed)

            model_instance = Model(nodes=nodes_list[0], x_train=x_train, y_train=growth, dropout=dropout, formulation=formulation)
            model_instance.fit(lr=lr, min_delta=min_delta, patience=patience, verbose=verbose)
            model_instance.in_sample_predictions()
            models_tmp[j] = model_instance

            #saves the average AIC and BIC values for each model configuration
            BIC_list[j] = model_instance.BIC
            AIC_list[j] = model_instance.AIC

            print(f"Process {os.getpid()} completed initialization {j+1}/{no_inits} (IC mode) for node {nodes_list[node_index]}", flush=True)

        # Select the best initialization based on BIC (or AIC)
        best_idx_BIC = int(np.argmin(BIC_list))
        best_idx_AIC = int(np.argmin(AIC_list))

        # Save the best model parameters for each model configuration
        
        models_tmp[best_idx_BIC].save_params('Model Parameters/' + formulation.capitalize() + '/BIC/' +  str(nodes_list[node_index]) + '.weights.h5')
        
        
        
        # if formulation =='global':
          
        #     # models_tmp[best_idx_AIC].save_params('Model Parameters/global/AIC/' + str(nodes_list[node_index]) + '.weights.h5')
        # else:
        #     for i in range(len(growth.keys())):
        #         models_tmp[best_idx_BIC, i].save_params('Model Parameters/regional/BIC/' + str(nodes_list[node_index]) + '_' + str(growth.keys(i)) + '.weights.h5')
        #         # models_tmp[best_idx_AIC, i].save_params('Model Parameters/regional/AIC/' + str(nodes_list[node_index]) + '_' + str(growth.keys(i)) + '.weights.h5')7
        #         
        return BIC_list[best_idx_BIC], AIC_list[best_idx_AIC], nodes_list[node_index]


def train_on_full_sample(best_init_idx, node, growth, precip, temp, lr, min_delta, patience, verbose, dropout):

        # ============ Retraining on Full Dataset Using the Best Seed ============
        # Compute the best seed based on the best initialization index.
        best_seed = best_init_idx

        # Clear session and reinitialize seeds for reproducibility using best_seed
        tf.keras.backend.clear_session()
        tf.random.set_seed(best_seed)
        np.random.default_rng(best_seed)
        random.seed(best_seed)

        x_train = [temp, precip]

        # Initialize the model on the full dataset with the same configuration as before.
        model_full = Model(nodes=node,
                        x_train=x_train,
                        y_train=growth,
                        dropout=dropout)

        # Train the model on the full dataset.
        model_full.fit(lr=lr, min_delta=min_delta, patience=patience, verbose=verbose)

        # Save the final model weights.
        model_full.save_params('Model Parameters/cv/' + str(node) + '.weights.h5')

        return None






