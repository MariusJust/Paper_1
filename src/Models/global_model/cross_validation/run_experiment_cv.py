import os
import pandas as pd
import numpy as np
import tensorflow as tf
import random
import warnings
from Models.model_functions import Prepare, load_data
from Models import MultivariateModelGlobal as Model
from panelsplit.cross_validation import PanelSplit
from panelsplit.plot import plot_splits
import logging

warnings.filterwarnings("ignore")
os.environ['PYTHONHASHSEED'] = str(0)



def main_loop(node_index, nodes_list, no_inits, seed_value, lr, min_delta, patience, verbose, dropout, n_splits, formulation):

    #load data
    growth, precip, temp, panel_split = load_data('CV', n_splits, formulation)
   
    # List to save model instances and performance metrics from each initialization.
    models_tmp =  np.zeros(no_inits, dtype=object)
    cv_errors_inits = np.zeros(no_inits)
    
    # Loop over initializations
    for j in range(no_inits):
        errors_j = []  # Store error (MSE) for each split in this initialization
        current_seed = seed_value + j  # update seed
        # iterate over each split generated by PanelSplit
        for train_idx, test_idx in panel_split.split():
            
            models_tmp[j]=train_itteration()

            mse=test_itteration(test_idx, models_tmp[j], growth, precip, temp)
            
            errors_j.append(mse)
            
        # Average error for this initialization
        cv_errors_inits[j] = np.mean(errors_j)
        
        logging.debug(f"Process {os.getpid()} completed initialization {j+1}/{no_inits} for node {nodes_list[node_index]}", flush=True)
            
        # Select best initialization (i.e., the one with the lowest average MSE)
        best_init_idx = int(np.argmin(cv_errors_inits))
        best_cv_error = cv_errors_inits[best_init_idx]
        
        #retrain the model on the full dataset using the best initialization
        train_on_full_sample()

        return best_cv_error, nodes_list[node_index]

    def train_itteration():
            # Prepare training and test sets
            growth_train = {'global': growth['global'].loc[train_idx]}

            # For temperature and precipitation:
            temp_train = {'global': temp['global'].loc[train_idx].reset_index(drop=True)}
          
            precip_train = {'global': precip['global'].loc[train_idx].reset_index(drop=True)}
         
            x_train = [temp_train, precip_train]
          
            # Clear session and set seeds for reproducibility
            tf.keras.backend.clear_session()
            tf.random.set_seed(current_seed)
            np.random.default_rng(current_seed)
            random.seed(current_seed)

            # Initialize and fit the model
            model_instance = Model(nodes=nodes_list[node_index], x_train=x_train, y_train=growth_train, dropout=dropout)
            model_instance.fit(lr=lr, min_delta=min_delta, patience=patience, verbose=verbose)
          
            return model_instance
        
    def test_itteration():
       # Prepare training and test sets
            growth_test = np.array(growth['global'].loc[test_idx].reset_index(drop=True)).reshape((1, 1, -1, 1))

            temp_test = np.array(temp['global'].loc[test_idx].reset_index(drop=True)).reshape((1, 1, -1, 1))

         
            precip_test = np.array(precip['global'].loc[test_idx].reset_index(drop=True)).reshape((1, 1, -1, 1))

            x_test = tf.concat([temp_test, precip_test], axis=3)
            
            preds = np.reshape(models_tmp[j].model_pred.predict(x_test), (-1, 1), order='F')
            mse = np.nanmean((preds - growth_test) ** 2)
            return mse
    
    def train_on_full_sample():

           
            # Compute the best seed based on the best initialization index.
            best_seed = best_init_idx

            # Clear session and reinitialize seeds for reproducibility using best_seed
            tf.keras.backend.clear_session()
            tf.random.set_seed(best_seed)
            np.random.default_rng(best_seed)
            random.seed(best_seed)

            x_train = [temp, precip]

            # Initialize the model on the full dataset with the same configuration as before.
            model_full = Model(nodes=nodes_list[node_index],
                            x_train=x_train,
                            y_train=growth,
                            dropout=dropout)

            # Train the model on the full dataset.
            model_full.fit(lr=lr, min_delta=min_delta, patience=patience, verbose=verbose)

            # Save the final model weights.
            model_full.save_params('Model Parameters/cv/' + str(nodes_list[node_index]) + '.weights.h5')

            return None

